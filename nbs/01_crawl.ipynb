{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl\n",
    "\n",
    "> A module to help you crawl policing websites and ingest them for later use with your LLM\n",
    "\n",
    "This notebook and the associated module and functions assist in scraping and cleaning website from domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to follow on from `police_risk_open_ai.core.llm`, and mostly replicates the opening stages of the [OpenAI embedding tutorial](https://platform.openai.com/docs/tutorials/web-qa-embeddings), though we make some code changes as we go.\n",
    "\n",
    "We start by replicating their code that helps scrape pages using beautifulsoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# Function to get the hyperlinks from a URL\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # If the response is not HTML, return an empty list\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # Decode the HTML\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # Parse the URL and check if the domain is the same\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))\n",
    "\n",
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = set([url])\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "\n",
    "        # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(url) # for debugging and to see the progress\n",
    "\n",
    "        # Save text from the url to a <url>.txt file\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "\n",
    "            # Get the text from the URL using BeautifulSoup\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # Get the text but remove the tags\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # Otherwise, write the text to the file in the text directory\n",
    "            f.write(text)\n",
    "\n",
    "        # Get the hyperlinks from the URL and add them to the queue\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above function \"out of the box\" on the College of Policing APP website though doesn't work as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.college.police.uk/app\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "domain = \"college.police.uk/app\" # <- put your domain to be crawled\n",
    "full_url = \"https://www.college.police.uk/app\" # <- put your domain to be crawled with https or http\n",
    "\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be a cloudflare response to prevent crawlers, but we can get it around it by modifying out user heading. Rather than requesting the URL directly, we'll inject in a header that looks like the Firefox browser.\n",
    "\n",
    "That said, when you do scrape websites, make sure you do it ethically: consider how much you're pulling in, what the audience is, and whether you might be impacting service for other users.  Given the APP is public and in high use, I feel that's okay here.\n",
    "\n",
    "```\n",
    "\n",
    "    request = urllib.request.Request(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    \n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(request) as response:\n",
    "```\n",
    "I also add in a max length of the URL, because the College has some real weird stuff that is breaking my code.\n",
    "\n",
    "```\n",
    "\n",
    "        # Save text from the url to a <url>.txt file\n",
    "        if len(url) < 500:\n",
    "            with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "That code does succesfully manage to run through the entirety of the APP domain, so we bundle it up below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# Function to get the hyperlinks from a URL\n",
    "def get_hyperlinks(url):\n",
    "\n",
    "    request = urllib.request.Request(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    \n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(request) as response:\n",
    "\n",
    "            # If the response is not HTML, return an empty list\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # Decode the HTML\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # Parse the URL and check if the domain is the same\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))\n",
    "\n",
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = set([url])\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "\n",
    "        # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(url) # for debugging and to see the progress\n",
    "\n",
    "        # Save text from the url to a <url>.txt file\n",
    "        if len(url) < 500:\n",
    "            with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "\n",
    "                # Get the text from the URL using BeautifulSoup\n",
    "                soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "                # Get the text but remove the tags\n",
    "                text = soup.get_text()\n",
    "\n",
    "                # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "                if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                    print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "                \n",
    "                # Otherwise, write the text to the file in the text directory\n",
    "                f.write(text)\n",
    "\n",
    "            # Get the hyperlinks from the URL and add them to the queue\n",
    "            for link in get_domain_hyperlinks(local_domain, url):\n",
    "                if link not in seen:\n",
    "                    queue.append(link)\n",
    "                    seen.add(link)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out on my website.  As you can see, loops through each hyperlink, and crawls it in turn, saving each as a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://andreasthinks.me/\n",
      "https://andreasthinks.me/./index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasthinksmint/python_env/police-risk-open-ai/lib/python3.8/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://andreasthinks.me/./recent_work.html\n",
      "https://andreasthinks.me/./posts/lapd_and_prophet/lapd_and_prophet.html\n",
      "https://andreasthinks.me/../../recent_work.html\n",
      "HTTP Error 400: Bad Request\n",
      "https://andreasthinks.me/../../index.xml\n",
      "HTTP Error 400: Bad Request\n",
      "https://andreasthinks.me/../../index.html\n",
      "HTTP Error 400: Bad Request\n",
      "https://andreasthinks.me/../../about.html\n",
      "HTTP Error 400: Bad Request\n",
      "https://andreasthinks.me/./posts/lockdown_effect/index.html\n",
      "https://andreasthinks.me/./posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html\n",
      "https://andreasthinks.me/./index.html\n",
      "https://andreasthinks.me/./posts/burglary_attendance/index.html\n",
      "https://andreasthinks.me/./about.html\n",
      "https://andreasthinks.me\n",
      "https://andreasthinks.me/./posts/migrated_to_quarto/index.html\n",
      "https://andreasthinks.me/./posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html\n"
     ]
    }
   ],
   "source": [
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "domain = \"andreasthinks.me\" # <- put your domain to be crawled\n",
    "full_url = \"https://andreasthinks.me/\" # <- put your domain to be crawled with https or http\n",
    "\n",
    "\n",
    "#crawl(full_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our API now scraped, we can move on to cleaning irrelevant data, and outputing something we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie\n",
    "\n",
    "def clean_scrapped_data(scrape_directory, output_file='processed/scraped.csv'):\n",
    "    \"\"\" takes a folder containing all the file from your scrapped data, cleans it all, saves as a CSV and returns the dataframe\n",
    "\n",
    "    if given None as output_file, dataframe returned but not saved\"\"\"\n",
    "\n",
    "    # Create a list to store the text files\n",
    "    texts=[]\n",
    "\n",
    "    # Get all the text files in the text directory\n",
    "    for file in os.listdir(scrape_directory + \"/\"):\n",
    "\n",
    "        # Open the file and read the text\n",
    "        with open(scrape_directory + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "            # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "            texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n",
    "\n",
    "    # Create a dataframe from the list of texts\n",
    "    df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "    # Set the text column to be the raw text with the newlines removed\n",
    "    df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "    if output_file is not None:\n",
    "        df.to_csv(output_file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_197341/3678221184.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  serie = serie.str.replace('\\\\n', ' ')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.police.uk app</td>\n",
       "      <td>.police.uk app.           APP (authorised prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.police.uk</td>\n",
       "      <td>.police.uk.            Working together | Coll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.police.uk about</td>\n",
       "      <td>.police.uk about.           About us | College...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.police.uk about concordats</td>\n",
       "      <td>.police.uk about concordats.           Concord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.police.uk about publication scheme</td>\n",
       "      <td>.police.uk about publication scheme.          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4441</th>\n",
       "      <td>.police.uk cdn cgi l email protection#cb8fedaa...</td>\n",
       "      <td>.police.uk cdn cgi l email protection#cb8fedaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>.police.uk cdn cgi l email protection#d3b7f5b2...</td>\n",
       "      <td>.police.uk cdn cgi l email protection#d3b7f5b2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4443</th>\n",
       "      <td>.police.uk cdn cgi l email protection#206f6446...</td>\n",
       "      <td>.police.uk cdn cgi l email protection#206f6446...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4444</th>\n",
       "      <td>.police.uk cdn cgi l email protection#97f4f8f9...</td>\n",
       "      <td>.police.uk cdn cgi l email protection#97f4f8f9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4445</th>\n",
       "      <td>.police.uk cdn cgi l email protection#02706771...</td>\n",
       "      <td>.police.uk cdn cgi l email protection#02706771...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4446 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  fname  \\\n",
       "0                                        .police.uk app   \n",
       "1                                            .police.uk   \n",
       "2                                      .police.uk about   \n",
       "3                           .police.uk about concordats   \n",
       "4                   .police.uk about publication scheme   \n",
       "...                                                 ...   \n",
       "4441  .police.uk cdn cgi l email protection#cb8fedaa...   \n",
       "4442  .police.uk cdn cgi l email protection#d3b7f5b2...   \n",
       "4443  .police.uk cdn cgi l email protection#206f6446...   \n",
       "4444  .police.uk cdn cgi l email protection#97f4f8f9...   \n",
       "4445  .police.uk cdn cgi l email protection#02706771...   \n",
       "\n",
       "                                                   text  \n",
       "0     .police.uk app.           APP (authorised prof...  \n",
       "1     .police.uk.            Working together | Coll...  \n",
       "2     .police.uk about.           About us | College...  \n",
       "3     .police.uk about concordats.           Concord...  \n",
       "4     .police.uk about publication scheme.          ...  \n",
       "...                                                 ...  \n",
       "4441  .police.uk cdn cgi l email protection#cb8fedaa...  \n",
       "4442  .police.uk cdn cgi l email protection#d3b7f5b2...  \n",
       "4443  .police.uk cdn cgi l email protection#206f6446...  \n",
       "4444  .police.uk cdn cgi l email protection#97f4f8f9...  \n",
       "4445  .police.uk cdn cgi l email protection#02706771...  \n",
       "\n",
       "[4446 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = clean_scrapped_data(\"text/www.college.police.uk\")\n",
    "cleaned_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there you have it! An entire website, scraped and cleaned, ready to be ingested into our AI model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
